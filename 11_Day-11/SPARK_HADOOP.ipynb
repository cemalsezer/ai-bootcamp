{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZS0S5IlKByG",
        "outputId": "5aacf9c4-eb7d-4e9c-af3d-6b45816279ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "def parseLine(line):\n",
        "  fields = line.split(',')\n",
        "  stationID=fields[0]\n",
        "  entryType=fields[2]\n",
        "  temperature=float(fields[3]) * 0.1 *(9.0 / 5.0) + 32.0\n",
        "  return (stationID, entryType, temperature)\n",
        "  \n",
        "lines = sc.textFile(\"1800.csv\")\n",
        "parsedLines = lines.map(parseLine)\n",
        "minTemps=parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
        "stationTemps=minTemps.map(lambda x: (x[0], x[2]))\n",
        "minTemps=stationTemps.reduceByKey(lambda x,y:min(x,y))\n",
        "results = minTemps.collect()\n",
        "\n",
        "for result in results:\n",
        "  print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_nsabVNKNQe",
        "outputId": "16c2bfeb-dae4-47ac-a281-69b32b6f4086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ITE00100554\t5.36F\n",
            "EZE00100082\t7.70F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"MaxTemperatures\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "def parseLine(line):\n",
        "  fields = line.split(',')\n",
        "  stationID = fields[0]\n",
        "  entryType = fields[2]\n",
        "  temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
        "  return (stationID, entryType, temperature)\n",
        "lines = sc.textFile(\"1800.csv\")\n",
        "parsedLines = lines.map(parseLine)\n",
        "minTemps = parsedLines.filter(lambda x: \"TMAX\" in x[1])\n",
        "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
        "minTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))\n",
        "results = minTemps.collect();\n",
        "  \n",
        "for result in results:\n",
        "  print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
        "  \n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNbN2kw0KVqA",
        "outputId": "4bba521a-284a-473d-9010-39fac267a5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ITE00100554\t90.14F\n",
            "EZE00100082\t90.14F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
        "sc = SparkContext(conf = conf)\n",
        "input = sc.textFile(\"book.txt\")\n",
        "words = input.flatMap(lambda x: x.split())\n",
        "wordCounts = words.countByValue()\n",
        "\n",
        "for word, count in wordCounts.items():\n",
        "  cleanWord = word.encode('ascii', 'ignore')\n",
        "  if (cleanWord):\n",
        "    print(cleanWord.decode() + \" \" + str(count))\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "3Z5hb5aQKwXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pyspark import SparkConf, SparkContext\n",
        "def normalizeWords(text):\n",
        "  return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
        "  conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
        "  sc = SparkContext(conf = conf)\n",
        "  input = sc.textFile(\"book.txt\")\n",
        "  words = input.flatMap(normalizeWords)\n",
        "  wordCounts = words.countByValue()\n",
        "\n",
        "for word, count in wordCounts.items():\n",
        "  cleanWord = word.encode('ascii', 'ignore')\n",
        "  if (cleanWord):\n",
        "    print(cleanWord.decode() + \" \" + str(count))\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "96MDzIQ5MA8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pyspark import SparkConf, SparkContext\n",
        "def normalizeWords(text):\n",
        "  return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
        "  \n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "input = sc.textFile(\"book.txt\")\n",
        "words = input.flatMap(normalizeWords)\n",
        "  \n",
        "wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey(False)\n",
        "results = wordCountsSorted.collect()\n",
        "\n",
        "for result in results:\n",
        "  count = str(result[0])\n",
        "  word = result[1].encode('ascii', 'ignore')\n",
        "  if (word):\n",
        "    print(word.decode() + \":\\t\\t\" + count)\n",
        "  \n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "yJsWfSlHNAjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"SpendByCustomerSorted\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "def extractCustomerPricePairs(line):\n",
        "  fields = line.split(',')\n",
        "  return (int(fields[0]), float(fields[2]))\n",
        "  \n",
        "input = sc.textFile(\"customer-orders.csv\")\n",
        "mappedInput = input.map(extractCustomerPricePairs)\n",
        "totalByCustomer = mappedInput.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "results = totalByCustomer.collect();\n",
        "for result in results:\n",
        "  print(result)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "c_PmZXQaNp3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "rHf7dtfjSGPM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"PopularMovies\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "lines = sc.textFile(\"u.data\")\n",
        "movies = lines.map(lambda x: (int(x.split()[1]), 1))\n",
        "movieCounts = movies.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "flipped = movieCounts.map( lambda xy : (xy[1], xy[0]) )\n",
        "sortedMovies = flipped.sortByKey(False) #(False) Descending order\n",
        "results = sortedMovies.collect()\n",
        "\n",
        "for result in results:\n",
        "  print(result)\n",
        "  \n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "vJG1PW9FRVB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "2B65xWbLT-Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "def loadMovieNames():\n",
        "  movieNames = {}\n",
        "  with open(\"u.item\", encoding = \"ISO-8859-1\") as f:\n",
        "    for line in f:\n",
        "      fields = line.split('|')\n",
        "      movieNames[int(fields[0])] = fields[1]\n",
        "    return movieNames\n",
        "    \n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"PopularMovies\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "nameDict = sc.broadcast(loadMovieNames())\n",
        "\n",
        "lines = sc.textFile(\"u.data\")\n",
        "movies = lines.map(lambda x: (int(x.split()[1]), 1))\n",
        "movieCounts = movies.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "flipped = movieCounts.map(lambda xy : (xy[1], xy[0]))\n",
        "sortedMovies = flipped.sortByKey(False)\n",
        "\n",
        "sortedMoviesWithNames = sortedMovies.map(lambda countMovie : (nameDict.value[countMovie[1]], countMovie[0]))\n",
        "\n",
        "results = sortedMoviesWithNames.collect()\n",
        "\n",
        "for result in results:\n",
        "  print(result)\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "pAgLyQlbU6R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"PopularHero\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "def countCoOccurences(line):\n",
        "  elements = line.split()\n",
        "  return (int(elements[0]), len(elements) - 1)\n",
        "\n",
        "def parseNames(line):\n",
        "  fields = line.split('\\\"')\n",
        "  return (int(fields[0]), fields[1].encode(\"utf8\"))\n",
        "  \n",
        "names = sc.textFile(\"Marvel-names.txt\")\n",
        "namesRdd = names.map(parseNames)\n",
        "lines = sc.textFile(\"Marvel-graph.txt\")\n",
        "\n",
        "pairings = lines.map(countCoOccurences)\n",
        "totalFriendsByCharacter = pairings.reduceByKey(lambda x, y : x + y)\n",
        "flipped = totalFriendsByCharacter.map(lambda xy : (xy[1], xy[0]))\n",
        "\n",
        "mostPopular = flipped.max()\n",
        "mostPopularName = namesRdd.lookup(mostPopular[1])[0]\n",
        "\n",
        "print(str(mostPopularName) + \" is the most popular superhero, with \" + str(mostPopular[0]) + \\\n",
        "      \" co-appearances.\")\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqKzF8EPVXqI",
        "outputId": "82aaf7da-3958-4ac2-a25a-8bb837594c2f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'CAPTAIN AMERICA' is the most popular superhero, with 1933 co-appearances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "VvUebt-AWNiG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "import collections\n",
        "# Create a SparkSession (Note, the config section is only for Windows!)\n",
        "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"\").appName(\"SparkSQL\").getOrCreate()\n",
        "\n",
        "def mapper(line):\n",
        "  fields = line.split(',')\n",
        "  return Row(ID=int(fields[0]), name=str(fields[1].encode(\"utf-8\")), age=int(fields[2]), numFriends=int(fields[3]))\n",
        "\n",
        "lines = spark.sparkContext.textFile(\"fakefriends.csv\")\n",
        "people = lines.map(mapper)\n",
        "# Infer the schema, and register the DataFrame as a table.\n",
        "schemaPeople = spark.createDataFrame(people).cache()\n",
        "schemaPeople.createOrReplaceTempView(\"people\")\n",
        "# SQL can be run over DataFrames that have been registered as a table.\n",
        "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")\n",
        "\n",
        "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
        "for teen in teenagers.collect():\n",
        "  print(teen)\n",
        "  # We can also use functions instead of SQL queries:\n",
        "  \n",
        "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "wWzttn98WVKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import functions\n",
        "\n",
        "def loadMovieNames():\n",
        "  movieNames = {}\n",
        "  with open(\"u.item\", encoding = \"ISO-8859-1\") as f:\n",
        "    for line in f:\n",
        "      fields = line.split('|')\n",
        "      movieNames[int(fields[0])] = fields[1]\n",
        "    return movieNames\n",
        "# Create a SparkSession (the config bit is only for Windows!)\n",
        "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"\").appName(\"PopularMovies\").getOrCreate()\n",
        "# Load up our movie ID -> name dictionary\n",
        "nameDict = loadMovieNames()\n",
        "# Get the raw data\n",
        "lines = spark.sparkContext.textFile(\"u.data\")\n",
        "# Convert it to a RDD of Row objects\n",
        "movies = lines.map(lambda x: Row(movieID =int(x.split()[1])))\n",
        "# Convert that to a DataFrame\n",
        "movieDataset = spark.createDataFrame(movies)\n",
        "# Some SQL-style magic to sort all movies by popularity in one line!\n",
        "topMovieIDs = movieDataset.groupBy(\"movieID\").count().orderBy(\"count\", ascending=False).cache()\n",
        "# Show the results at this point:#|movieID|co\n",
        "\n",
        "topMovieIDs.show()\n",
        "\n",
        "top10=topMovieIDs.take(10)\n",
        "\n",
        "print(\"\\n\")\n",
        "for result in top10:\n",
        "    print(\"%s: %d\" % (nameDict[result[0]], result[1]))\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjWMZm2RX7YR",
        "outputId": "38c4629a-4bad-4cda-81ec-a1611e87c3db"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|movieID|count|\n",
            "+-------+-----+\n",
            "|     50|  584|\n",
            "|    258|  509|\n",
            "|    100|  508|\n",
            "|    181|  507|\n",
            "|    294|  485|\n",
            "|    286|  481|\n",
            "|    288|  478|\n",
            "|      1|  452|\n",
            "|    300|  431|\n",
            "|    121|  429|\n",
            "|    174|  420|\n",
            "|    127|  413|\n",
            "|     56|  394|\n",
            "|      7|  392|\n",
            "|     98|  390|\n",
            "|    237|  384|\n",
            "|    117|  378|\n",
            "|    172|  368|\n",
            "|    222|  365|\n",
            "|    313|  350|\n",
            "+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "\n",
            "Star Wars (1977): 584\n",
            "Contact (1997): 509\n",
            "Fargo (1996): 508\n",
            "Return of the Jedi (1983): 507\n",
            "Liar Liar (1997): 485\n",
            "English Patient, The (1996): 481\n",
            "Scream (1996): 478\n",
            "Toy Story (1995): 452\n",
            "Air Force One (1997): 431\n",
            "Independence Day (ID4) (1996): 429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HkqVmUwEcQQZ"
      }
    }
  ]
}